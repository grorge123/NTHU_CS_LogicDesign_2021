{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s37cJXAYGrTY"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - I**\n",
    "\n",
    "*   define actor critic networks\n",
    "*   define PPO algorithm\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT6VUBg-F8Zm",
    "outputId": "d2c595b5-4f8b-4e92-f1f2-7d1ee4a3775d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "============================================================================================\n",
      "Device set to : NVIDIA GeForce RTX 3070\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################### Import libraries ###############################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import pongGym\n",
    "\n",
    "################################## set device ##################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    \n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "#         self.quant = torch.quantization.QuantStub()\n",
    "        self.actor = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, action_dim),\n",
    "                        nn.Softmax(dim=-1)\n",
    "                    )\n",
    "\n",
    "#         self.dequant = torch.quantization.DeQuantStub()\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 1)\n",
    "                    )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        \n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init=0.6):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action, action_logprob = self.policy_old.act(state)\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr-ZjT_CGyEi"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - II**\n",
    "\n",
    "*   train PPO algorithm on environments\n",
    "*   save preTrained networks weights and log files\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY1-DzVCF8eh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "training environment name : pong_game\n",
      "current logging run number for pong_game :  10\n",
      "logging at : PPO_logs/pong_game//PPO_pong_game_log_10.csv\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000\n",
      "max timesteps per episode :  3600\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 7200 timesteps\n",
      "printing average reward over episodes in last : 14400 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  22\n",
      "action space dimension :  4\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a discrete action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 14400 timesteps\n",
      "PPO K epochs :  180\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2021-12-16 20:51:07\n",
      "============================================================================================\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_28_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 3 \t\t Timestep : 14400 \t\t Average Reward : -39.8\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_192_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:14\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 7 \t\t Timestep : 28800 \t\t Average Reward : 2.94\n",
      "Episode : 11 \t\t Timestep : 43200 \t\t Average Reward : 25.53\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_250_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:42\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 15 \t\t Timestep : 57600 \t\t Average Reward : -50.32\n",
      "Episode : 19 \t\t Timestep : 72000 \t\t Average Reward : -43.4\n",
      "Episode : 23 \t\t Timestep : 86400 \t\t Average Reward : -80.87\n",
      "Episode : 27 \t\t Timestep : 100800 \t\t Average Reward : 3.1\n",
      "Episode : 31 \t\t Timestep : 115200 \t\t Average Reward : -29.03\n",
      "Episode : 35 \t\t Timestep : 129600 \t\t Average Reward : 139.6\n",
      "Episode : 39 \t\t Timestep : 144000 \t\t Average Reward : -61.82\n",
      "Episode : 43 \t\t Timestep : 158400 \t\t Average Reward : -29.07\n",
      "Episode : 47 \t\t Timestep : 172800 \t\t Average Reward : -46.22\n",
      "Episode : 51 \t\t Timestep : 187200 \t\t Average Reward : -149.98\n",
      "Episode : 55 \t\t Timestep : 201600 \t\t Average Reward : -192.73\n",
      "Episode : 59 \t\t Timestep : 216000 \t\t Average Reward : -102.25\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_608_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:44\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 63 \t\t Timestep : 230400 \t\t Average Reward : 208.62\n",
      "Episode : 67 \t\t Timestep : 244800 \t\t Average Reward : 4.29\n",
      "Episode : 71 \t\t Timestep : 259200 \t\t Average Reward : -124.77\n",
      "Episode : 75 \t\t Timestep : 273600 \t\t Average Reward : 123.32\n",
      "Episode : 79 \t\t Timestep : 288000 \t\t Average Reward : -37.14\n",
      "Episode : 83 \t\t Timestep : 302400 \t\t Average Reward : -20.34\n",
      "Episode : 87 \t\t Timestep : 316800 \t\t Average Reward : -237.4\n",
      "Episode : 91 \t\t Timestep : 331200 \t\t Average Reward : -190.17\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_649_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:03:20\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 95 \t\t Timestep : 345600 \t\t Average Reward : 179.43\n",
      "Episode : 99 \t\t Timestep : 360000 \t\t Average Reward : 80.11\n",
      "Episode : 103 \t\t Timestep : 374400 \t\t Average Reward : -3.68\n",
      "Episode : 107 \t\t Timestep : 388800 \t\t Average Reward : 114.61\n",
      "Episode : 111 \t\t Timestep : 403200 \t\t Average Reward : -39.64\n",
      "Episode : 115 \t\t Timestep : 417600 \t\t Average Reward : -102.59\n",
      "Episode : 119 \t\t Timestep : 432000 \t\t Average Reward : 83.59\n",
      "Episode : 123 \t\t Timestep : 446400 \t\t Average Reward : 107.49\n",
      "Episode : 127 \t\t Timestep : 460800 \t\t Average Reward : 9.05\n",
      "Episode : 131 \t\t Timestep : 475200 \t\t Average Reward : 50.1\n",
      "Episode : 135 \t\t Timestep : 489600 \t\t Average Reward : 57.82\n",
      "Episode : 139 \t\t Timestep : 504000 \t\t Average Reward : 14.06\n",
      "Episode : 143 \t\t Timestep : 518400 \t\t Average Reward : 164.24\n",
      "Episode : 147 \t\t Timestep : 532800 \t\t Average Reward : 17.17\n",
      "Episode : 151 \t\t Timestep : 547200 \t\t Average Reward : -56.09\n",
      "Episode : 155 \t\t Timestep : 561600 \t\t Average Reward : -266.9\n",
      "Episode : 159 \t\t Timestep : 576000 \t\t Average Reward : -69.87\n",
      "Episode : 163 \t\t Timestep : 590400 \t\t Average Reward : 23.86\n",
      "Episode : 167 \t\t Timestep : 604800 \t\t Average Reward : 238.93\n",
      "Episode : 171 \t\t Timestep : 619200 \t\t Average Reward : -304.11\n",
      "Episode : 175 \t\t Timestep : 633600 \t\t Average Reward : 36.08\n",
      "Episode : 179 \t\t Timestep : 648000 \t\t Average Reward : 6.07\n",
      "Episode : 183 \t\t Timestep : 662400 \t\t Average Reward : 218.02\n",
      "Episode : 187 \t\t Timestep : 676800 \t\t Average Reward : 229.34\n",
      "Episode : 191 \t\t Timestep : 691200 \t\t Average Reward : 48.74\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_656_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:06:16\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 195 \t\t Timestep : 705600 \t\t Average Reward : 314.29\n",
      "Episode : 199 \t\t Timestep : 720000 \t\t Average Reward : 72.49\n",
      "Episode : 203 \t\t Timestep : 734400 \t\t Average Reward : 159.79\n",
      "Episode : 207 \t\t Timestep : 748800 \t\t Average Reward : 174.78\n",
      "Episode : 211 \t\t Timestep : 763200 \t\t Average Reward : -16.71\n",
      "Episode : 215 \t\t Timestep : 777600 \t\t Average Reward : 330.97\n",
      "Episode : 219 \t\t Timestep : 792000 \t\t Average Reward : 37.7\n",
      "Episode : 223 \t\t Timestep : 806400 \t\t Average Reward : 26.54\n",
      "Episode : 227 \t\t Timestep : 820800 \t\t Average Reward : 95.44\n",
      "Episode : 231 \t\t Timestep : 835200 \t\t Average Reward : 288.92\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_680_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:06:59\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 235 \t\t Timestep : 849600 \t\t Average Reward : 293.19\n",
      "Episode : 239 \t\t Timestep : 864000 \t\t Average Reward : 162.48\n",
      "Episode : 243 \t\t Timestep : 878400 \t\t Average Reward : 10.48\n",
      "Episode : 247 \t\t Timestep : 892800 \t\t Average Reward : -61.45\n",
      "Episode : 251 \t\t Timestep : 907200 \t\t Average Reward : -11.24\n",
      "Episode : 255 \t\t Timestep : 921600 \t\t Average Reward : -75.52\n",
      "Episode : 259 \t\t Timestep : 936000 \t\t Average Reward : 347.81\n",
      "Episode : 263 \t\t Timestep : 950400 \t\t Average Reward : 57.45\n",
      "Episode : 267 \t\t Timestep : 964800 \t\t Average Reward : 227.57\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_794_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:07:40\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 271 \t\t Timestep : 979200 \t\t Average Reward : 188.97\n",
      "Episode : 275 \t\t Timestep : 993600 \t\t Average Reward : -6.54\n",
      "Episode : 279 \t\t Timestep : 1008000 \t\t Average Reward : 181.23\n",
      "Episode : 283 \t\t Timestep : 1022400 \t\t Average Reward : 337.98\n",
      "Episode : 287 \t\t Timestep : 1036800 \t\t Average Reward : 245.41\n",
      "Episode : 291 \t\t Timestep : 1051200 \t\t Average Reward : 108.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 295 \t\t Timestep : 1065600 \t\t Average Reward : 131.65\n",
      "Episode : 299 \t\t Timestep : 1080000 \t\t Average Reward : 477.5\n",
      "Episode : 303 \t\t Timestep : 1094400 \t\t Average Reward : -102.6\n",
      "Episode : 307 \t\t Timestep : 1108800 \t\t Average Reward : -156.42\n",
      "Episode : 311 \t\t Timestep : 1123200 \t\t Average Reward : 72.89\n",
      "Episode : 315 \t\t Timestep : 1137600 \t\t Average Reward : 423.76\n",
      "Episode : 319 \t\t Timestep : 1152000 \t\t Average Reward : 120.85\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_889_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:08:41\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 323 \t\t Timestep : 1166400 \t\t Average Reward : 402.92\n",
      "Episode : 327 \t\t Timestep : 1180800 \t\t Average Reward : 122.12\n",
      "Episode : 331 \t\t Timestep : 1195200 \t\t Average Reward : 182.86\n",
      "Episode : 335 \t\t Timestep : 1209600 \t\t Average Reward : 323.7\n",
      "Episode : 339 \t\t Timestep : 1224000 \t\t Average Reward : 66.91\n",
      "Episode : 343 \t\t Timestep : 1238400 \t\t Average Reward : 30.25\n",
      "Episode : 347 \t\t Timestep : 1252800 \t\t Average Reward : 222.66\n",
      "Episode : 351 \t\t Timestep : 1267200 \t\t Average Reward : 63.3\n",
      "Episode : 355 \t\t Timestep : 1281600 \t\t Average Reward : -60.54\n",
      "Episode : 359 \t\t Timestep : 1296000 \t\t Average Reward : 100.85\n",
      "Episode : 363 \t\t Timestep : 1310400 \t\t Average Reward : 152.69\n",
      "Episode : 367 \t\t Timestep : 1324800 \t\t Average Reward : 86.4\n",
      "Episode : 371 \t\t Timestep : 1339200 \t\t Average Reward : 342.8\n",
      "Episode : 375 \t\t Timestep : 1353600 \t\t Average Reward : 254.34\n",
      "Episode : 379 \t\t Timestep : 1368000 \t\t Average Reward : -47.32\n",
      "Episode : 383 \t\t Timestep : 1382400 \t\t Average Reward : 172.09\n",
      "Episode : 387 \t\t Timestep : 1396800 \t\t Average Reward : 62.03\n",
      "Episode : 391 \t\t Timestep : 1411200 \t\t Average Reward : -178.04\n",
      "Episode : 395 \t\t Timestep : 1425600 \t\t Average Reward : 182.29\n",
      "Episode : 399 \t\t Timestep : 1440000 \t\t Average Reward : 211.17\n",
      "Episode : 403 \t\t Timestep : 1454400 \t\t Average Reward : 345.0\n",
      "Episode : 407 \t\t Timestep : 1468800 \t\t Average Reward : 204.28\n",
      "Episode : 411 \t\t Timestep : 1483200 \t\t Average Reward : 28.79\n",
      "Episode : 415 \t\t Timestep : 1497600 \t\t Average Reward : 158.06\n",
      "Episode : 419 \t\t Timestep : 1512000 \t\t Average Reward : 191.03\n",
      "Episode : 423 \t\t Timestep : 1526400 \t\t Average Reward : -70.57\n",
      "Episode : 427 \t\t Timestep : 1540800 \t\t Average Reward : 129.2\n",
      "Episode : 431 \t\t Timestep : 1555200 \t\t Average Reward : 240.41\n",
      "Episode : 435 \t\t Timestep : 1569600 \t\t Average Reward : 88.57\n",
      "Episode : 439 \t\t Timestep : 1584000 \t\t Average Reward : 236.29\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_960_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:10:54\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 443 \t\t Timestep : 1598400 \t\t Average Reward : 209.14\n",
      "Episode : 447 \t\t Timestep : 1612800 \t\t Average Reward : 11.47\n",
      "Episode : 451 \t\t Timestep : 1627200 \t\t Average Reward : 307.25\n",
      "Episode : 455 \t\t Timestep : 1641600 \t\t Average Reward : 390.0\n",
      "Episode : 459 \t\t Timestep : 1656000 \t\t Average Reward : 334.7\n",
      "Episode : 463 \t\t Timestep : 1670400 \t\t Average Reward : -31.8\n",
      "Episode : 467 \t\t Timestep : 1684800 \t\t Average Reward : 394.59\n",
      "Episode : 471 \t\t Timestep : 1699200 \t\t Average Reward : -17.02\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_996_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:11:31\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 475 \t\t Timestep : 1713600 \t\t Average Reward : 281.57\n",
      "Episode : 479 \t\t Timestep : 1728000 \t\t Average Reward : 269.97\n",
      "Episode : 483 \t\t Timestep : 1742400 \t\t Average Reward : 354.73\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/pong_game/PPO_pong_game_1090_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:11:42\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 487 \t\t Timestep : 1756800 \t\t Average Reward : 310.81\n",
      "Episode : 491 \t\t Timestep : 1771200 \t\t Average Reward : 235.22\n",
      "Episode : 495 \t\t Timestep : 1785600 \t\t Average Reward : 353.19\n",
      "Episode : 499 \t\t Timestep : 1800000 \t\t Average Reward : 228.75\n",
      "Episode : 503 \t\t Timestep : 1814400 \t\t Average Reward : 496.03\n",
      "Episode : 507 \t\t Timestep : 1828800 \t\t Average Reward : 265.52\n",
      "Episode : 511 \t\t Timestep : 1843200 \t\t Average Reward : 446.59\n",
      "Episode : 515 \t\t Timestep : 1857600 \t\t Average Reward : 124.54\n",
      "Episode : 519 \t\t Timestep : 1872000 \t\t Average Reward : 394.37\n",
      "Episode : 523 \t\t Timestep : 1886400 \t\t Average Reward : 100.74\n",
      "Episode : 527 \t\t Timestep : 1900800 \t\t Average Reward : 414.89\n",
      "Episode : 531 \t\t Timestep : 1915200 \t\t Average Reward : 192.11\n",
      "Episode : 535 \t\t Timestep : 1929600 \t\t Average Reward : 506.09\n",
      "Episode : 539 \t\t Timestep : 1944000 \t\t Average Reward : 350.75\n",
      "Episode : 543 \t\t Timestep : 1958400 \t\t Average Reward : 195.11\n",
      "Episode : 547 \t\t Timestep : 1972800 \t\t Average Reward : 572.7\n",
      "Episode : 551 \t\t Timestep : 1987200 \t\t Average Reward : 316.12\n",
      "Episode : 555 \t\t Timestep : 2001600 \t\t Average Reward : 42.48\n",
      "Episode : 559 \t\t Timestep : 2016000 \t\t Average Reward : 240.54\n",
      "Episode : 563 \t\t Timestep : 2030400 \t\t Average Reward : 410.41\n",
      "Episode : 567 \t\t Timestep : 2044800 \t\t Average Reward : 152.9\n",
      "Episode : 571 \t\t Timestep : 2059200 \t\t Average Reward : 77.36\n",
      "Episode : 575 \t\t Timestep : 2073600 \t\t Average Reward : 95.02\n",
      "Episode : 579 \t\t Timestep : 2088000 \t\t Average Reward : 319.77\n",
      "Episode : 583 \t\t Timestep : 2102400 \t\t Average Reward : 406.74\n",
      "Episode : 587 \t\t Timestep : 2116800 \t\t Average Reward : 115.88\n",
      "Episode : 591 \t\t Timestep : 2131200 \t\t Average Reward : 206.89\n",
      "Episode : 595 \t\t Timestep : 2145600 \t\t Average Reward : 410.3\n",
      "Episode : 599 \t\t Timestep : 2160000 \t\t Average Reward : 105.59\n",
      "Episode : 603 \t\t Timestep : 2174400 \t\t Average Reward : 430.18\n",
      "Episode : 607 \t\t Timestep : 2188800 \t\t Average Reward : 77.02\n",
      "Episode : 611 \t\t Timestep : 2203200 \t\t Average Reward : 341.23\n",
      "Episode : 615 \t\t Timestep : 2217600 \t\t Average Reward : 301.32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"pong_game\"\n",
    "\n",
    "max_ep_len = 60*60                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 60*3               # update policy for K epochs\n",
    "# K_epochs = 4\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = pongGym.DoublePong()\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "Max_reward = 0\n",
    "# training loop\n",
    "# env = pongGym.DoublePong()\n",
    "while i_episode < 100000:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "#         if i_episode > 100:\n",
    "#             env.render()\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "            \n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # save model weights\n",
    "#     print(\"Episode : {} \\t\\t Totol reward : {}\".format(i_episode, current_ep_reward))\n",
    "    if current_ep_reward > Max_reward:\n",
    "        Max_reward = current_ep_reward\n",
    "        checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, int(Max_reward), run_num_pretrained)\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        print(\"saving model at : \" + checkpoint_path)\n",
    "        ppo_agent.save(checkpoint_path)\n",
    "        print(\"model saved\")\n",
    "        print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, \"final\", run_num_pretrained)\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"saving model at : \" + checkpoint_path)\n",
    "ppo_agent.save(checkpoint_path)\n",
    "print(\"model saved\")\n",
    "print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "print(\"--------------------------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PPO_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
